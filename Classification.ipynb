{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Project 3 : Buzz Prediction on Twitter\n",
    "\n",
    "Project Description:\n",
    "- Use same datasets as Project 2.\n",
    "- Run all the models only on 10% data. Use code given in Project 2 for sampling.\n",
    "- Preprocess data: Explore data and apply data scaling.\n",
    "\n",
    "Regression Task:\n",
    "- Apply any two models with bagging and any two models with pasting.\n",
    "- Apply any two models with adaboost boosting\n",
    "- Apply one model with gradient boosting\n",
    "- Apply PCA on data and then apply all the models in project 2 again on data you get from PCA. Compare your results with results in project 2. You don't need to apply all the models twice. Just copy the result table from project 2, prepare similar table for all the models after PCA and compare both tables. Does PCA help in getting better results?\n",
    "- Apply deep learning models covered in class\n",
    "\n",
    "Classification Task:\n",
    "- Apply four voting classifiers - two with hard voting and two with soft voting\n",
    "- Apply any two models with bagging and any two models with pasting.\n",
    "- Apply any two models with adaboost boosting\n",
    "- Apply one model with gradient boosting\n",
    "- Apply PCA on data and then apply all the models in project 2 again on data you get from PCA. Compare your results with results in project 2. You don't need to apply all the models twice. Just copy the result table from project 2, prepare similar table for all the models after PCA and compare both tables. Does PCA help in getting better results?\n",
    "- Apply deep learning models covered in class\n",
    "\n",
    "Deliverables:\n",
    "- Use markdown to provide inline comments for this project.\n",
    "- Your outputs should be clearly executed in the notebook i.e. we should not need to rerun the code to obtain the outputs.\n",
    "- Visualization encouraged.\n",
    "- If you are submitting two different files, then please only one group member submit both the files. If you submit two files separately from different accounts, it will be submitted as two different attempts.\n",
    "- If you are submitting two different files, then please follow below naming convetion:\n",
    "    Project3_Regression_GroupXX_Firstname1_Firstname2.ipynb\n",
    "    Project3_Classification_GroupXX_Firstname1_Firstname2.ipynb\n",
    "- If you are submitting single file, then please follow below naming convetion:\n",
    "    Project3_Both_GroupXX_Firstname1_Firstname2.ipynb\n",
    "\n",
    "Questions regarding the project:\n",
    "- We have created a discussion board under Projects folder on e-learning. Create threads over there and post your queries related to project there.\n",
    "- We will also answer queries there. We will not be answering any project related queries through the mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=['NCD_0', 'NCD_1', 'NCD_2', 'NCD_3', 'NCD_4', 'NCD_5', 'NCD_6',\n",
    "       'AI_0', 'AI_1', 'AI_2', 'AI_3', 'AI_4','AI_5', 'AI_6','AS(NA)_0',\n",
    "       'AS(NA)_1', 'AS(NA)_2', 'AS(NA)_3', 'AS(NA)_4','AS(NA)_5', 'AS(NA)_6','BL_0',\n",
    "       'BL_1', 'BL_2', 'BL_3', 'BL_4', 'BL_5', 'BL_6', 'NAC_0', \n",
    "       'NAC_1', 'NAC_2', 'NAC_3', 'NAC_4', 'NAC_5', 'NAC_6','AS(NAC)_0',\n",
    "       'AS(NAC)_1', 'AS(NAC)_2', 'AS(NAC)_3', 'AS(NAC)_4','AS(NAC)_5', 'AS(NAC)_6',\n",
    "       'CS_0', 'CS_1', 'CS_2', 'CS_3', 'CS_4', 'CS_5', 'CS_6', \n",
    "       'AT_0', 'AT_1', 'AT_2', 'AT_3', 'AT_4', 'AT_5', 'AT_6','NA_0', 'NA_1', 'NA_2', 'NA_3', 'NA_4', 'NA_5', 'NA_6',\n",
    "       'ADL_0', 'ADL_1', 'ADL_2', 'ADL_3','ADL_4', 'ADL_5', 'ADL_6','NAD_0', 'NAD_1', 'NAD_2','NAD_3', 'NAD_4', 'NAD_5', \n",
    "       'NAD_6','Buzz']\n",
    "twt = pd.read_csv('Twitter-Absolute-Sigma-500.data',names = index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NCD_0</th>\n",
       "      <th>NCD_1</th>\n",
       "      <th>NCD_2</th>\n",
       "      <th>NCD_3</th>\n",
       "      <th>NCD_4</th>\n",
       "      <th>NCD_5</th>\n",
       "      <th>NCD_6</th>\n",
       "      <th>AI_0</th>\n",
       "      <th>AI_1</th>\n",
       "      <th>AI_2</th>\n",
       "      <th>...</th>\n",
       "      <th>ADL_5</th>\n",
       "      <th>ADL_6</th>\n",
       "      <th>NAD_0</th>\n",
       "      <th>NAD_1</th>\n",
       "      <th>NAD_2</th>\n",
       "      <th>NAD_3</th>\n",
       "      <th>NAD_4</th>\n",
       "      <th>NAD_5</th>\n",
       "      <th>NAD_6</th>\n",
       "      <th>Buzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>889</td>\n",
       "      <td>939</td>\n",
       "      <td>960</td>\n",
       "      <td>805</td>\n",
       "      <td>805</td>\n",
       "      <td>1143</td>\n",
       "      <td>1121</td>\n",
       "      <td>549</td>\n",
       "      <td>613</td>\n",
       "      <td>587</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>889</td>\n",
       "      <td>939</td>\n",
       "      <td>960</td>\n",
       "      <td>805</td>\n",
       "      <td>805</td>\n",
       "      <td>1143</td>\n",
       "      <td>1121</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NCD_0  NCD_1  NCD_2  NCD_3  NCD_4  NCD_5  NCD_6  AI_0  AI_1  AI_2  ...   \\\n",
       "0    889    939    960    805    805   1143   1121   549   613   587  ...    \n",
       "\n",
       "   ADL_5  ADL_6  NAD_0  NAD_1  NAD_2  NAD_3  NAD_4  NAD_5  NAD_6  Buzz  \n",
       "0    1.0    1.0    889    939    960    805    805   1143   1121   1.0  \n",
       "\n",
       "[1 rows x 78 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140707 entries, 0 to 140706\n",
      "Data columns (total 78 columns):\n",
      "NCD_0        140707 non-null int64\n",
      "NCD_1        140707 non-null int64\n",
      "NCD_2        140707 non-null int64\n",
      "NCD_3        140707 non-null int64\n",
      "NCD_4        140707 non-null int64\n",
      "NCD_5        140707 non-null int64\n",
      "NCD_6        140707 non-null int64\n",
      "AI_0         140707 non-null int64\n",
      "AI_1         140707 non-null int64\n",
      "AI_2         140707 non-null int64\n",
      "AI_3         140707 non-null int64\n",
      "AI_4         140707 non-null int64\n",
      "AI_5         140707 non-null int64\n",
      "AI_6         140707 non-null int64\n",
      "AS(NA)_0     140707 non-null float64\n",
      "AS(NA)_1     140707 non-null float64\n",
      "AS(NA)_2     140707 non-null float64\n",
      "AS(NA)_3     140707 non-null float64\n",
      "AS(NA)_4     140707 non-null float64\n",
      "AS(NA)_5     140707 non-null float64\n",
      "AS(NA)_6     140707 non-null float64\n",
      "BL_0         140707 non-null float64\n",
      "BL_1         140707 non-null float64\n",
      "BL_2         140707 non-null float64\n",
      "BL_3         140707 non-null float64\n",
      "BL_4         140707 non-null float64\n",
      "BL_5         140707 non-null float64\n",
      "BL_6         140707 non-null float64\n",
      "NAC_0        140707 non-null int64\n",
      "NAC_1        140707 non-null int64\n",
      "NAC_2        140707 non-null int64\n",
      "NAC_3        140707 non-null int64\n",
      "NAC_4        140707 non-null int64\n",
      "NAC_5        140707 non-null int64\n",
      "NAC_6        140707 non-null int64\n",
      "AS(NAC)_0    140707 non-null float64\n",
      "AS(NAC)_1    140707 non-null float64\n",
      "AS(NAC)_2    140707 non-null float64\n",
      "AS(NAC)_3    140707 non-null float64\n",
      "AS(NAC)_4    140707 non-null float64\n",
      "AS(NAC)_5    140707 non-null float64\n",
      "AS(NAC)_6    140707 non-null float64\n",
      "CS_0         140707 non-null float64\n",
      "CS_1         140707 non-null float64\n",
      "CS_2         140707 non-null float64\n",
      "CS_3         140707 non-null float64\n",
      "CS_4         140707 non-null float64\n",
      "CS_5         140707 non-null float64\n",
      "CS_6         140707 non-null float64\n",
      "AT_0         140707 non-null float64\n",
      "AT_1         140707 non-null float64\n",
      "AT_2         140707 non-null float64\n",
      "AT_3         140707 non-null float64\n",
      "AT_4         140707 non-null float64\n",
      "AT_5         140707 non-null float64\n",
      "AT_6         140707 non-null float64\n",
      "NA_0         140707 non-null int64\n",
      "NA_1         140707 non-null int64\n",
      "NA_2         140707 non-null int64\n",
      "NA_3         140707 non-null int64\n",
      "NA_4         140707 non-null int64\n",
      "NA_5         140707 non-null int64\n",
      "NA_6         140707 non-null int64\n",
      "ADL_0        140707 non-null float64\n",
      "ADL_1        140707 non-null float64\n",
      "ADL_2        140707 non-null float64\n",
      "ADL_3        140707 non-null float64\n",
      "ADL_4        140707 non-null float64\n",
      "ADL_5        140707 non-null float64\n",
      "ADL_6        140707 non-null float64\n",
      "NAD_0        140707 non-null int64\n",
      "NAD_1        140707 non-null int64\n",
      "NAD_2        140707 non-null int64\n",
      "NAD_3        140707 non-null int64\n",
      "NAD_4        140707 non-null int64\n",
      "NAD_5        140707 non-null int64\n",
      "NAD_6        140707 non-null int64\n",
      "Buzz         140707 non-null float64\n",
      "dtypes: float64(43), int64(35)\n",
      "memory usage: 83.7 MB\n"
     ]
    }
   ],
   "source": [
    "twt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NCD_0</th>\n",
       "      <th>NCD_1</th>\n",
       "      <th>NCD_2</th>\n",
       "      <th>NCD_3</th>\n",
       "      <th>NCD_4</th>\n",
       "      <th>NCD_5</th>\n",
       "      <th>NCD_6</th>\n",
       "      <th>AI_0</th>\n",
       "      <th>AI_1</th>\n",
       "      <th>AI_2</th>\n",
       "      <th>...</th>\n",
       "      <th>ADL_5</th>\n",
       "      <th>ADL_6</th>\n",
       "      <th>NAD_0</th>\n",
       "      <th>NAD_1</th>\n",
       "      <th>NAD_2</th>\n",
       "      <th>NAD_3</th>\n",
       "      <th>NAD_4</th>\n",
       "      <th>NAD_5</th>\n",
       "      <th>NAD_6</th>\n",
       "      <th>Buzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "      <td>140707.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>172.279823</td>\n",
       "      <td>155.150625</td>\n",
       "      <td>165.464476</td>\n",
       "      <td>176.820549</td>\n",
       "      <td>186.937700</td>\n",
       "      <td>216.209208</td>\n",
       "      <td>243.866510</td>\n",
       "      <td>87.050154</td>\n",
       "      <td>78.639236</td>\n",
       "      <td>84.269574</td>\n",
       "      <td>...</td>\n",
       "      <td>1.113444</td>\n",
       "      <td>1.196131</td>\n",
       "      <td>172.838807</td>\n",
       "      <td>155.630878</td>\n",
       "      <td>165.938674</td>\n",
       "      <td>177.314810</td>\n",
       "      <td>187.463794</td>\n",
       "      <td>216.776294</td>\n",
       "      <td>244.479194</td>\n",
       "      <td>0.197396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>509.872276</td>\n",
       "      <td>471.573236</td>\n",
       "      <td>495.360236</td>\n",
       "      <td>528.351277</td>\n",
       "      <td>560.331281</td>\n",
       "      <td>632.188378</td>\n",
       "      <td>707.402192</td>\n",
       "      <td>234.731748</td>\n",
       "      <td>218.448179</td>\n",
       "      <td>233.536510</td>\n",
       "      <td>...</td>\n",
       "      <td>1.374287</td>\n",
       "      <td>1.826150</td>\n",
       "      <td>510.937549</td>\n",
       "      <td>472.462733</td>\n",
       "      <td>496.233557</td>\n",
       "      <td>529.286514</td>\n",
       "      <td>561.309487</td>\n",
       "      <td>633.203935</td>\n",
       "      <td>708.436795</td>\n",
       "      <td>0.398035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>125.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.119048</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24210.000000</td>\n",
       "      <td>22899.000000</td>\n",
       "      <td>20495.000000</td>\n",
       "      <td>27007.000000</td>\n",
       "      <td>30957.000000</td>\n",
       "      <td>28603.000000</td>\n",
       "      <td>37505.000000</td>\n",
       "      <td>15105.000000</td>\n",
       "      <td>15730.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>185.666672</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>24301.000000</td>\n",
       "      <td>22980.000000</td>\n",
       "      <td>20495.000000</td>\n",
       "      <td>27071.000000</td>\n",
       "      <td>31028.000000</td>\n",
       "      <td>28697.000000</td>\n",
       "      <td>37505.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               NCD_0          NCD_1          NCD_2          NCD_3  \\\n",
       "count  140707.000000  140707.000000  140707.000000  140707.000000   \n",
       "mean      172.279823     155.150625     165.464476     176.820549   \n",
       "std       509.872276     471.573236     495.360236     528.351277   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         3.000000       2.000000       3.000000       3.000000   \n",
       "50%        22.000000      19.000000      20.000000      22.000000   \n",
       "75%       125.000000     112.000000     119.000000     126.000000   \n",
       "max     24210.000000   22899.000000   20495.000000   27007.000000   \n",
       "\n",
       "               NCD_4          NCD_5          NCD_6           AI_0  \\\n",
       "count  140707.000000  140707.000000  140707.000000  140707.000000   \n",
       "mean      186.937700     216.209208     243.866510      87.050154   \n",
       "std       560.331281     632.188378     707.402192     234.731748   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         3.000000       4.000000       5.000000       2.000000   \n",
       "50%        23.000000      28.000000      33.000000      13.000000   \n",
       "75%       133.000000     161.000000     186.000000      70.000000   \n",
       "max     30957.000000   28603.000000   37505.000000   15105.000000   \n",
       "\n",
       "                AI_1           AI_2      ...                ADL_5  \\\n",
       "count  140707.000000  140707.000000      ...        140707.000000   \n",
       "mean       78.639236      84.269574      ...             1.113444   \n",
       "std       218.448179     233.536510      ...             1.374287   \n",
       "min         0.000000       0.000000      ...             0.000000   \n",
       "25%         2.000000       2.000000      ...             1.000000   \n",
       "50%        11.000000      13.000000      ...             1.000000   \n",
       "75%        64.000000      67.000000      ...             1.100000   \n",
       "max     15730.000000   16389.000000      ...           185.666672   \n",
       "\n",
       "               ADL_6          NAD_0          NAD_1          NAD_2  \\\n",
       "count  140707.000000  140707.000000  140707.000000  140707.000000   \n",
       "mean        1.196131     172.838807     155.630878     165.938674   \n",
       "std         1.826150     510.937549     472.462733     496.233557   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         1.000000       3.000000       2.000000       3.000000   \n",
       "50%         1.000000      22.000000      19.000000      21.000000   \n",
       "75%         1.119048     126.000000     113.000000     119.000000   \n",
       "max       295.000000   24301.000000   22980.000000   20495.000000   \n",
       "\n",
       "               NAD_3          NAD_4          NAD_5          NAD_6  \\\n",
       "count  140707.000000  140707.000000  140707.000000  140707.000000   \n",
       "mean      177.314810     187.463794     216.776294     244.479194   \n",
       "std       529.286514     561.309487     633.203935     708.436795   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         3.000000       3.000000       4.000000       6.000000   \n",
       "50%        22.000000      23.000000      28.000000      33.000000   \n",
       "75%       127.000000     134.000000     162.000000     187.000000   \n",
       "max     27071.000000   31028.000000   28697.000000   37505.000000   \n",
       "\n",
       "                Buzz  \n",
       "count  140707.000000  \n",
       "mean        0.197396  \n",
       "std         0.398035  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 78 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCD_0       0\n",
       "NCD_1       0\n",
       "NCD_2       0\n",
       "NCD_3       0\n",
       "NCD_4       0\n",
       "NCD_5       0\n",
       "NCD_6       0\n",
       "AI_0        0\n",
       "AI_1        0\n",
       "AI_2        0\n",
       "AI_3        0\n",
       "AI_4        0\n",
       "AI_5        0\n",
       "AI_6        0\n",
       "AS(NA)_0    0\n",
       "AS(NA)_1    0\n",
       "AS(NA)_2    0\n",
       "AS(NA)_3    0\n",
       "AS(NA)_4    0\n",
       "AS(NA)_5    0\n",
       "AS(NA)_6    0\n",
       "BL_0        0\n",
       "BL_1        0\n",
       "BL_2        0\n",
       "BL_3        0\n",
       "BL_4        0\n",
       "BL_5        0\n",
       "BL_6        0\n",
       "NAC_0       0\n",
       "NAC_1       0\n",
       "           ..\n",
       "CS_6        0\n",
       "AT_0        0\n",
       "AT_1        0\n",
       "AT_2        0\n",
       "AT_3        0\n",
       "AT_4        0\n",
       "AT_5        0\n",
       "AT_6        0\n",
       "NA_0        0\n",
       "NA_1        0\n",
       "NA_2        0\n",
       "NA_3        0\n",
       "NA_4        0\n",
       "NA_5        0\n",
       "NA_6        0\n",
       "ADL_0       0\n",
       "ADL_1       0\n",
       "ADL_2       0\n",
       "ADL_3       0\n",
       "ADL_4       0\n",
       "ADL_5       0\n",
       "ADL_6       0\n",
       "NAD_0       0\n",
       "NAD_1       0\n",
       "NAD_2       0\n",
       "NAD_3       0\n",
       "NAD_4       0\n",
       "NAD_5       0\n",
       "NAD_6       0\n",
       "Buzz        0\n",
       "Length: 78, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = twt['Buzz']\n",
    "X = twt.drop('Buzz',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NCD_0</th>\n",
       "      <th>NCD_1</th>\n",
       "      <th>NCD_2</th>\n",
       "      <th>NCD_3</th>\n",
       "      <th>NCD_4</th>\n",
       "      <th>NCD_5</th>\n",
       "      <th>NCD_6</th>\n",
       "      <th>AI_0</th>\n",
       "      <th>AI_1</th>\n",
       "      <th>AI_2</th>\n",
       "      <th>...</th>\n",
       "      <th>ADL_4</th>\n",
       "      <th>ADL_5</th>\n",
       "      <th>ADL_6</th>\n",
       "      <th>NAD_0</th>\n",
       "      <th>NAD_1</th>\n",
       "      <th>NAD_2</th>\n",
       "      <th>NAD_3</th>\n",
       "      <th>NAD_4</th>\n",
       "      <th>NAD_5</th>\n",
       "      <th>NAD_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>889</td>\n",
       "      <td>939</td>\n",
       "      <td>960</td>\n",
       "      <td>805</td>\n",
       "      <td>805</td>\n",
       "      <td>1143</td>\n",
       "      <td>1121</td>\n",
       "      <td>549</td>\n",
       "      <td>613</td>\n",
       "      <td>587</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>889</td>\n",
       "      <td>939</td>\n",
       "      <td>960</td>\n",
       "      <td>805</td>\n",
       "      <td>805</td>\n",
       "      <td>1143</td>\n",
       "      <td>1121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NCD_0  NCD_1  NCD_2  NCD_3  NCD_4  NCD_5  NCD_6  AI_0  AI_1  AI_2  ...    \\\n",
       "0    889    939    960    805    805   1143   1121   549   613   587  ...     \n",
       "\n",
       "   ADL_4  ADL_5  ADL_6  NAD_0  NAD_1  NAD_2  NAD_3  NAD_4  NAD_5  NAD_6  \n",
       "0    1.0    1.0    1.0    889    939    960    805    805   1143   1121  \n",
       "\n",
       "[1 rows x 77 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "Name: Buzz, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sample_X1, sample_X2, sample_y1,sample_y2 = train_test_split(X, y, shuffle = True, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14071, 77)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14071,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sample_X2, sample_y2, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Apply four voting classifiers - two with hard voting and two with soft voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1) hard voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('knn', KNeigh...         min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier(7)\n",
    "svm_clf = SVC(C = 10, probability = True)\n",
    "dt = DecisionTreeClassifier(max_depth = 5,random_state= 0)\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('knn', knn_clf), ('svc', svm_clf),('dt',dt)], voting='hard')\n",
    "voting_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9471814306016106\n",
      "KNeighborsClassifier 0.9616295594504974\n",
      "SVC 0.9483657034580767\n",
      "DecisionTreeClassifier 0.9644718143060161\n",
      "VotingClassifier 0.9478919943154903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, knn_clf, svm_clf, dt,voting_clf):\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)  hard voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth = 5,random_state= 0)\n",
    "svm = SVC(random_state=0,kernel='rbf')\n",
    "log_clf2 = LogisticRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('dt', Decisio...bf',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf2), ('dt', dtc), ('svc', svm)], voting='hard')\n",
    "voting_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.9644718143060161\n",
      "SVC 0.9078635717669351\n",
      "LogisticRegression 0.9471814306016106\n",
      "VotingClassifier 0.94860255802937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (dtc, svm, log_clf2, voting_clf):\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1)  soft voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('knn', KNeigh...         min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "knn_clf = KNeighborsClassifier(7)\n",
    "svm_clf = SVC(C = 10, probability = True)\n",
    "dt = DecisionTreeClassifier(max_depth = 5,random_state= 0)\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('knn', knn_clf), ('svc', svm_clf),('dt',dt)], voting='soft')\n",
    "voting_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9471814306016106\n",
      "KNeighborsClassifier 0.9616295594504974\n",
      "SVC 0.9483657034580767\n",
      "DecisionTreeClassifier 0.9644718143060161\n",
      "VotingClassifier 0.9623401231643771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, knn_clf, svm_clf, dt, voting_clf):\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2)soft voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.9644718143060161\n",
      "SVC 0.9078635717669351\n",
      "LogisticRegression 0.9471814306016106\n",
      "VotingClassifier 0.94860255802937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth = 5,random_state= 0)\n",
    "svm = SVC(random_state=0,kernel='rbf')\n",
    "log_clf2 = LogisticRegression()\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf2), ('dt', dtc), ('svc', svm)], voting='hard')\n",
    "voting_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (dtc, svm, log_clf2, voting_clf):\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Apply any two models with bagging and any two models with pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) KNN bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 25)\n",
    "\n",
    "bag_clf = BaggingClassifier(knn, n_estimators=500, max_samples=200, bootstrap=True, n_jobs=-1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=25, p=2,\n",
       "           weights='uniform'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=200, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "         random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bag_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9424443391757461\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2) logistic bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(penalty = 'l1', C = 1,random_state=0)\n",
    "bag_log = BaggingClassifier(log_clf, n_estimators=500, max_samples=200, bootstrap=True, n_jobs=-1, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=200, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "         random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_log.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = bag_log.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8562292752250118\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1) pasting with DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth = 5,random_state= 0)\n",
    "pas_dt = BaggingClassifier(dt, n_estimators=500, max_samples=200, bootstrap=False, n_jobs=-1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'),\n",
       "         bootstrap=False, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=200, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "         random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pas_dt.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = pas_dt.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9663666508763619\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2) Pasting with LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc_lin = LinearSVC(C =10,penalty='l2',random_state=0)\n",
    "pas_svc = BaggingClassifier(svc_lin, n_estimators=500, max_samples=200, bootstrap=False, n_jobs=-1, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
       "     verbose=0),\n",
       "         bootstrap=False, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=200, n_estimators=500, n_jobs=-1, oob_score=False,\n",
       "         random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pas_svc.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred4 = pas_svc.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9639981051634297\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Apply any two models with adaboost boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Adaboost Classifier with DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred5 = ada_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9637612505921365\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2) Adaboost Classifier with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf2 = AdaBoostClassifier(LogisticRegression(penalty = 'l1', C = 1), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf2.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred6 = ada_clf2.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7889625769777356\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Apply one model with gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.01, loss='deviance', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbrt = GradientBoostingClassifier(random_state=0,max_depth=5,learning_rate=0.01)\n",
    "gbrt.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred7 = gbrt.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.961155850307911\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape:    (14071, 77)\n",
      "transformed shape: (14071, 77)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(sample_X2)\n",
    "X_pca = pca.transform(sample_X2)\n",
    "print(\"original shape:   \", sample_X2.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, sample_y2, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "report2 = pd.DataFrame(index = None,columns = ['Model name', 'Model parameter', 'Train accuracy', 'Test accuracy', 'Train auc score', 'Test auc score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9855361999174483"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors':[5,10,15,20,25]}\n",
    "\n",
    "grid_knn = GridSearchCV(knn, param_grid=param_grid, cv = 5, scoring='roc_auc', return_train_score=True)\n",
    "grid_knn.fit(X_train_scaled, y_train)\n",
    "j = grid_knn.score(X_train_scaled, y_train)\n",
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9610495746276186"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = grid_knn.score(X_test_scaled, y_test)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_knn_predict = grid_knn.predict(X_test_scaled)\n",
    "y_knn_train_predict = grid_knn.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train roc_auc_score: 0.93\n",
      "Test roc_auc_score: 0.93 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('Train roc_auc_score: %.2f'%roc_auc_score(y_knn_train_predict, y_train))\n",
    "print('Test roc_auc_score: %.2f '%roc_auc_score(y_knn_predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "report2 = report2.append(pd.Series({'Model name':'KNN Classifier', 'Model parameter':grid_knn.best_params_, 'Train accuracy':j, 'Test accuracy':k, 'Train auc score':roc_auc_score(y_knn_train_predict, y_train), 'Test auc score':roc_auc_score(y_knn_predict, y_test)}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Model parameter</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Test accuracy</th>\n",
       "      <th>Train auc score</th>\n",
       "      <th>Test auc score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>{'n_neighbors': 25}</td>\n",
       "      <td>0.985536</td>\n",
       "      <td>0.96105</td>\n",
       "      <td>0.929823</td>\n",
       "      <td>0.926065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model name      Model parameter  Train accuracy  Test accuracy  \\\n",
       "0  KNN Classifier  {'n_neighbors': 25}        0.985536        0.96105   \n",
       "\n",
       "   Train auc score  Test auc score  \n",
       "0         0.929823        0.926065  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2']}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "\n",
    "param_grid = {'penalty':['l1', 'l2']}\n",
    "\n",
    "grid_log = GridSearchCV(log_clf , param_grid, cv = 5, return_train_score=True, scoring='roc_auc')\n",
    "grid_log.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9911236284866554"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = grid_log.score(X_train_scaled, y_train)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925994276727657"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = grid_log.score(X_test_scaled, y_test)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_log_predict = grid_log.predict(X_test_scaled)\n",
    "y_log_train_predict = grid_log.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train roc_auc_score: 0.96\n",
      "Test roc_auc_score: 0.95 \n"
     ]
    }
   ],
   "source": [
    "print('Train roc_auc_score: %.2f'%roc_auc_score(y_log_train_predict, y_train))\n",
    "print('Test roc_auc_score: %.2f '%roc_auc_score(y_log_predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "report2 = report2.append(pd.Series({'Model name':'Logistic Regression', 'Model parameter':grid_log.best_params_, 'Train accuracy':l, 'Test accuracy':m, 'Train auc score':roc_auc_score(y_log_train_predict, y_train), 'Test auc score':roc_auc_score(y_log_predict, y_test)}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Model parameter</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Test accuracy</th>\n",
       "      <th>Train auc score</th>\n",
       "      <th>Test auc score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>{'n_neighbors': 25}</td>\n",
       "      <td>0.985536</td>\n",
       "      <td>0.961050</td>\n",
       "      <td>0.929823</td>\n",
       "      <td>0.926065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'penalty': 'l1'}</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.992599</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.953378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model name      Model parameter  Train accuracy  Test accuracy  \\\n",
       "0       KNN Classifier  {'n_neighbors': 25}        0.985536       0.961050   \n",
       "1  Logistic Regression    {'penalty': 'l1'}        0.991124       0.992599   \n",
       "\n",
       "   Train auc score  Test auc score  \n",
       "0         0.929823        0.926065  \n",
       "1         0.955329        0.953378  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [5, 10, 20, 50, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "param_grid = {'max_depth': [5, 10, 20, 50, 100]}\n",
    "\n",
    "grid_dt = GridSearchCV(dt_clf, param_grid, cv = 5,scoring = 'roc_auc', return_train_score=True)\n",
    "grid_dt.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922253749650906"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = grid_dt.score(X_train_scaled, y_train)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861443537978428"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = grid_dt.score(X_test_scaled, y_test)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dt_predict = grid_dt.predict(X_test_scaled)\n",
    "y_dt_train_predict = grid_dt.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train roc_auc_score: 0.95\n",
      "Test roc_auc_score: 0.94 \n"
     ]
    }
   ],
   "source": [
    "print('Train roc_auc_score: %.2f'%roc_auc_score(y_dt_train_predict, y_train))\n",
    "print('Test roc_auc_score: %.2f '%roc_auc_score(y_dt_predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "report2 = report2.append(pd.Series({'Model name':'Decision Tree', 'Model parameter':grid_dt.best_params_, 'Train accuracy':n, 'Test accuracy':o, 'Train auc score':roc_auc_score(y_dt_train_predict, y_train), 'Test auc score':roc_auc_score(y_dt_predict, y_test)}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Model parameter</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Test accuracy</th>\n",
       "      <th>Train auc score</th>\n",
       "      <th>Test auc score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>{'n_neighbors': 25}</td>\n",
       "      <td>0.985536</td>\n",
       "      <td>0.961050</td>\n",
       "      <td>0.929823</td>\n",
       "      <td>0.926065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'penalty': 'l1'}</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.992599</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.953378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>0.992225</td>\n",
       "      <td>0.986144</td>\n",
       "      <td>0.953877</td>\n",
       "      <td>0.942553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model name      Model parameter  Train accuracy  Test accuracy  \\\n",
       "0       KNN Classifier  {'n_neighbors': 25}        0.985536       0.961050   \n",
       "1  Logistic Regression    {'penalty': 'l1'}        0.991124       0.992599   \n",
       "2        Decision Tree     {'max_depth': 5}        0.992225       0.986144   \n",
       "\n",
       "   Train auc score  Test auc score  \n",
       "0         0.929823        0.926065  \n",
       "1         0.955329        0.953378  \n",
       "2         0.953877        0.942553  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc_lin = LinearSVC()\n",
    "param_grid = {'C':[ 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_svc_lin = GridSearchCV(svc_lin, param_grid, cv = 5, scoring='roc_auc', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10, 100]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score=True, scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc_lin.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992065390723765"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = grid_svc_lin.score(X_train_scaled, y_train)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992978578327735"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = grid_svc_lin.score(X_test_scaled, y_test)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svc_lin_predict_train = grid_svc_lin.predict(X_train_scaled)\n",
    "y_svc_lin_predict = grid_svc_lin.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train roc_auc_score: 0.96\n",
      "Test roc_auc_score: 0.96 \n"
     ]
    }
   ],
   "source": [
    "print('Train roc_auc_score: %.2f'%roc_auc_score(y_svc_lin_predict_train, y_train))\n",
    "print('Test roc_auc_score: %.2f '%roc_auc_score(y_svc_lin_predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "report2 = report2.append(pd.Series({'Model name':'Linear Support Vactor Machine', 'Model parameter':grid_svc_lin.best_params_, 'Train accuracy':p, 'Test accuracy':q, 'Train auc score':roc_auc_score(y_svc_lin_predict_train, y_train), 'Test auc score':roc_auc_score(y_svc_lin_predict, y_test)}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Model parameter</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Test accuracy</th>\n",
       "      <th>Train auc score</th>\n",
       "      <th>Test auc score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>{'n_neighbors': 25}</td>\n",
       "      <td>0.985536</td>\n",
       "      <td>0.961050</td>\n",
       "      <td>0.929823</td>\n",
       "      <td>0.926065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'penalty': 'l1'}</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.992599</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.953378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>0.992225</td>\n",
       "      <td>0.986144</td>\n",
       "      <td>0.953877</td>\n",
       "      <td>0.942553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear Support Vactor Machine</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.992065</td>\n",
       "      <td>0.992979</td>\n",
       "      <td>0.962448</td>\n",
       "      <td>0.960270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model name      Model parameter  Train accuracy  \\\n",
       "0                 KNN Classifier  {'n_neighbors': 25}        0.985536   \n",
       "1            Logistic Regression    {'penalty': 'l1'}        0.991124   \n",
       "2                  Decision Tree     {'max_depth': 5}        0.992225   \n",
       "3  Linear Support Vactor Machine            {'C': 10}        0.992065   \n",
       "\n",
       "   Test accuracy  Train auc score  Test auc score  \n",
       "0       0.961050         0.929823        0.926065  \n",
       "1       0.992599         0.955329        0.953378  \n",
       "2       0.986144         0.953877        0.942553  \n",
       "3       0.992979         0.962448        0.960270  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "D:\\UTD\\sem 3\\python\\Anaconda\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10, 50], 'max_iter': [1000, 10000], 'gamma': [0.01, 0.1, 0.5, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "param_grid = {'C':[ 0.01, 0.1,1, 10, 50], 'max_iter':[1000,10000], 'gamma':[0.01, 0.1, 0.5, 1]}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(random_state=0,kernel='rbf'), param_grid, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9684231901715911"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = grid_search.score(X_train_scaled, y_train)\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9658929417337755"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = grid_search.score(X_test_scaled, y_test)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_knn_predict = grid_search.predict(X_test_scaled)\n",
    "y_knn_train_predict = grid_search.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train roc_auc_score: 0.96\n",
      "Test roc_auc_score: 0.95 \n"
     ]
    }
   ],
   "source": [
    "print('Train roc_auc_score: %.2f'%roc_auc_score(y_knn_train_predict, y_train))\n",
    "print('Test roc_auc_score: %.2f '%roc_auc_score(y_knn_predict, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "report2 = report2.append(pd.Series({'Model name':'Kernelized Support Vector Machine', 'Model parameter':grid_search.best_params_, 'Train accuracy':y1, 'Test accuracy':z, 'Train auc score':roc_auc_score(y_knn_train_predict, y_train), 'Test auc score':roc_auc_score(y_knn_predict, y_test)}),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Model parameter</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Test accuracy</th>\n",
       "      <th>Train auc score</th>\n",
       "      <th>Test auc score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN Classifier</td>\n",
       "      <td>{'n_neighbors': 25}</td>\n",
       "      <td>0.985536</td>\n",
       "      <td>0.961050</td>\n",
       "      <td>0.929823</td>\n",
       "      <td>0.926065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'penalty': 'l1'}</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.992599</td>\n",
       "      <td>0.955329</td>\n",
       "      <td>0.953378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>{'max_depth': 5}</td>\n",
       "      <td>0.992225</td>\n",
       "      <td>0.986144</td>\n",
       "      <td>0.953877</td>\n",
       "      <td>0.942553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear Support Vactor Machine</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>0.992065</td>\n",
       "      <td>0.992979</td>\n",
       "      <td>0.962448</td>\n",
       "      <td>0.960270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kernelized Support Vector Machine</td>\n",
       "      <td>{'C': 50, 'gamma': 0.5, 'max_iter': 1000}</td>\n",
       "      <td>0.968423</td>\n",
       "      <td>0.965893</td>\n",
       "      <td>0.957554</td>\n",
       "      <td>0.952875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model name  \\\n",
       "0                     KNN Classifier   \n",
       "1                Logistic Regression   \n",
       "2                      Decision Tree   \n",
       "3      Linear Support Vactor Machine   \n",
       "4  Kernelized Support Vector Machine   \n",
       "\n",
       "                             Model parameter  Train accuracy  Test accuracy  \\\n",
       "0                        {'n_neighbors': 25}        0.985536       0.961050   \n",
       "1                          {'penalty': 'l1'}        0.991124       0.992599   \n",
       "2                           {'max_depth': 5}        0.992225       0.986144   \n",
       "3                                  {'C': 10}        0.992065       0.992979   \n",
       "4  {'C': 50, 'gamma': 0.5, 'max_iter': 1000}        0.968423       0.965893   \n",
       "\n",
       "   Train auc score  Test auc score  \n",
       "0         0.929823        0.926065  \n",
       "1         0.955329        0.953378  \n",
       "2         0.953877        0.942553  \n",
       "3         0.962448        0.960270  \n",
       "4         0.957554        0.952875  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modelname=['KNN Classifier','Logistic Regression','Decision Tree','Linear Support Vactor Machine','Kernelized Support Vactor Machine']\n",
    "Training_accuracy=[0.992926,.991839,.991359,0.992107,.968931]\n",
    "Testing_accuracy = [0.986681,0.991953,0.979446,0.992116,0.963051]\n",
    "Train_auc_score = [0.954012,0.958533,0.967647,0.958805,0.957723]\n",
    "Test_auc_score = [0.953212,0.958540,0.946589,0.956889,0.951082]\n",
    "\n",
    "accuracy=pd.DataFrame({'Modelname':Modelname,'Training accuracy':Training_accuracy,'Testing accuracy':Testing_accuracy,'Train auc score':Train_auc_score,'Test auc score':Test_auc_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=77, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(64,  kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(64,  kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(32,  kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1,  kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "9849/9849 [==============================] - 2s 155us/step - loss: 0.1657\n",
      "Epoch 2/20\n",
      "9849/9849 [==============================] - 0s 40us/step - loss: 0.1592\n",
      "Epoch 3/20\n",
      "9849/9849 [==============================] - 0s 40us/step - loss: 0.1576\n",
      "Epoch 4/20\n",
      "9849/9849 [==============================] - 0s 42us/step - loss: 0.1499\n",
      "Epoch 5/20\n",
      "9849/9849 [==============================] - 0s 44us/step - loss: 0.1166\n",
      "Epoch 6/20\n",
      "9849/9849 [==============================] - 0s 46us/step - loss: 0.0958\n",
      "Epoch 7/20\n",
      "9849/9849 [==============================] - 1s 77us/step - loss: 0.0840\n",
      "Epoch 8/20\n",
      "9849/9849 [==============================] - 1s 61us/step - loss: 0.0474\n",
      "Epoch 9/20\n",
      "9849/9849 [==============================] - 1s 71us/step - loss: 0.0499: 1s - lo\n",
      "Epoch 10/20\n",
      "9849/9849 [==============================] - 0s 47us/step - loss: 0.0515\n",
      "Epoch 11/20\n",
      "9849/9849 [==============================] - 0s 43us/step - loss: 0.0406\n",
      "Epoch 12/20\n",
      "9849/9849 [==============================] - 0s 47us/step - loss: 0.0404\n",
      "Epoch 13/20\n",
      "9849/9849 [==============================] - 1s 90us/step - loss: 0.0364\n",
      "Epoch 14/20\n",
      "9849/9849 [==============================] - 0s 48us/step - loss: 0.0315\n",
      "Epoch 15/20\n",
      "9849/9849 [==============================] - 0s 46us/step - loss: 0.0326\n",
      "Epoch 16/20\n",
      "9849/9849 [==============================] - 0s 41us/step - loss: 0.0316\n",
      "Epoch 17/20\n",
      "9849/9849 [==============================] - 0s 48us/step - loss: 0.0762\n",
      "Epoch 18/20\n",
      "9849/9849 [==============================] - 1s 85us/step - loss: 0.0431\n",
      "Epoch 19/20\n",
      "9849/9849 [==============================] - 0s 48us/step - loss: 0.0329\n",
      "Epoch 20/20\n",
      "9849/9849 [==============================] - 0s 49us/step - loss: 0.0337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x144ceec3e10>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs = 20, batch_size = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.70\n",
      "Test score: 0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, recall_score, precision_score\n",
    "\n",
    "y_train_predict = model.predict(X_train_scaled)\n",
    "y_test_predict = model.predict(X_test_scaled)\n",
    "\n",
    "print('Train score: {:.2f}'.format(r2_score(y_train, y_train_predict)))\n",
    "print('Test score: {:.2f}'.format(r2_score(y_test, y_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
